{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_generation_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitanya-kh/text_generation_lstm/blob/master/Text_generation_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo9Ck3RwfoEv",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnC6ZBEEjG9V",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Following is covered i this book:\n",
        "\n",
        "\n",
        "1.   Getting the text data which is a novel 'Tom Sawyer'\n",
        "2.   Tokenizing the text data at character level using keras pre-processing\n",
        "3.    Preparing the data for training\n",
        "4.   Building model with Embed, LSTM and Dense layers\n",
        "5.   Training the model and checking the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AIy_oARf4lK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_xQ2u3-k10B",
        "colab_type": "text"
      },
      "source": [
        "## Getting the text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFIXqubgkTbC",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 41
        },
        "outputId": "f02ba5a3-ec0b-4935-b384-dc2f70e98d47"
      },
      "source": [
        "#Uploading the novel 'The adventured of Tom Sawyer' from https://www.gutenberg.org/ebooks/74\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aa4c9b9b-c343-45f2-b8d2-aad2ef03d3ea\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-aa4c9b9b-c343-45f2-b8d2-aad2ef03d3ea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "800iUID8nJY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7092b128-337e-4978-f04d-5739f552a388"
      },
      "source": [
        "f = open('tom.txt', 'r')\n",
        "text = ''.join(f.readlines())\n",
        "print ('Length of text: ',len(text))\n",
        "off = np.random.randint(0, len(text)-5000, size=(4,), )\n",
        "\n",
        "#Prepare dataset as a list with entire text as first entry and \n",
        "#next entries as random 2000 length sequences from the text which can be used as input for text generation\n",
        "dataset = [text, text[off[0]:off[0]+2000], text[off[1]:off[1]+2000], text[off[2]:off[2]+2000], text[off[3]:off[3]+2000]]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text:  386669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQmVA-r1o5z-",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing the text data at character level using keras pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X-bcW47pM7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1780f390-7156-4bfa-eed2-673f2bb9b897"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcLOZWzpom9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorize the entire text\n",
        "t = Tokenizer(lower=True, char_level=True)\n",
        "t.fit_on_texts(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMvVZLK6pLI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "da23af61-1c88-4ef6-aa0f-e189ddcfeaf5"
      },
      "source": [
        "#Checkout the character encoding\n",
        "t.word_index"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 14,\n",
              " ' ': 1,\n",
              " '!': 31,\n",
              " '&': 48,\n",
              " \"'\": 26,\n",
              " '(': 42,\n",
              " ')': 43,\n",
              " '*': 45,\n",
              " ',': 22,\n",
              " '-': 28,\n",
              " '.': 24,\n",
              " '2': 44,\n",
              " ':': 37,\n",
              " ';': 33,\n",
              " '?': 34,\n",
              " '[': 40,\n",
              " ']': 41,\n",
              " '_': 35,\n",
              " 'a': 4,\n",
              " 'b': 21,\n",
              " 'c': 19,\n",
              " 'd': 11,\n",
              " 'e': 2,\n",
              " 'f': 20,\n",
              " 'g': 18,\n",
              " 'h': 7,\n",
              " 'i': 8,\n",
              " 'j': 32,\n",
              " 'k': 25,\n",
              " 'l': 12,\n",
              " 'm': 16,\n",
              " 'n': 6,\n",
              " 'o': 5,\n",
              " 'p': 23,\n",
              " 'q': 38,\n",
              " 'r': 10,\n",
              " 's': 9,\n",
              " 't': 3,\n",
              " 'u': 13,\n",
              " 'v': 27,\n",
              " 'w': 15,\n",
              " 'x': 36,\n",
              " 'y': 17,\n",
              " 'z': 39,\n",
              " '\\xa0': 46,\n",
              " '“': 29,\n",
              " '”': 30,\n",
              " '\\ufeff': 47}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1MXQMK6tPmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3e230e4-47fc-4d4b-f142-41e347a2d308"
      },
      "source": [
        "vocab_size = len(t.word_index)+1 #0 is not used in word index\n",
        "print (vocab_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76YoNyI-t2Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert dataset to tokens\n",
        "tokens = t.texts_to_sequences(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYx_l5PWuh97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1ce9e8e-acb6-436c-cc93-8c3624e9a3a4"
      },
      "source": [
        "dataset_len = len(tokens[0])\n",
        "print (dataset_len)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "386669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXrB-0t9raYL",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTGG6szDr62p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "09162935-0aa1-4208-ee94-a8da8269ef2c"
      },
      "source": [
        "#Define batch size to be passed in .fit()\n",
        "batch_size=32\n",
        "\n",
        "#Define samples per batch\n",
        "samp_per_batch = 128\n",
        "\n",
        "#Calculate number of batches based on above\n",
        "num_batches = dataset_len - samp_per_batch - 1 #int((dataset_len-1)/samp_per_batch) \n",
        "num_batches = num_batches - (num_batches%batch_size)\n",
        "#max_l = samp_per_batch * num_batches\n",
        "\n",
        "print ('Text length: ', dataset_len)\n",
        "print ('Samples per batch: ', samp_per_batch)\n",
        "print ('Number of batches: ', num_batches)\n",
        "#print ('Text length considered for training: ', max_l)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text length:  386669\n",
            "Samples per batch:  128\n",
            "Number of batches:  386528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_1BxAeevEIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7999b436-5685-4bca-a80a-36680a91ca21"
      },
      "source": [
        "#Prepare input and output according to the samples per batch and total number of batches\n",
        "\n",
        "x_inp = []\n",
        "y_op = []\n",
        "\n",
        "for i in range(num_batches):\n",
        "  x_inp.append(tokens[0][i:i+samp_per_batch])\n",
        "  y_op.append(tokens[0][i+samp_per_batch+1])\n",
        "\n",
        "#One hot encode the y_op\n",
        "y_op = np.array(keras.utils.to_categorical(y_op, vocab_size))\n",
        "\n",
        "#Reshape x_inp to make it compatible for Embed layer\n",
        "x_inp = np.array(x_inp).reshape(num_batches, samp_per_batch) \n",
        "\n",
        "#Feature scaling\n",
        "#x_inp = x_inp/vocab_size\n",
        "\n",
        "print ('Input shape: ', x_inp.shape)\n",
        "print ('Output shape: ', y_op.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape:  (386528, 128)\n",
            "Output shape:  (386528, 49)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEalF4P2raS8",
        "colab_type": "text"
      },
      "source": [
        "## Building model with Embed, LSTM and Dense layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3wzyLLSr78V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, GRU, Dense, CuDNNGRU, TimeDistributed, CuDNNLSTM, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1s4SLeEx6Vq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "cff55e3b-63f2-4f11-e2de-ab4118b7e688"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "#Keras Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "#Add embed layer which translates our sample vector to an embedding. Embedding is constructed such that \n",
        "#characters closer to each other wil be placed at lesser distances compared to others\n",
        "model.add(Embedding(vocab_size, 8, input_length=samp_per_batch, batch_input_shape=(batch_size, samp_per_batch)))\n",
        "\n",
        "#Add an GPU based LSTM layer with 750 units\n",
        "#stateful to be set as True so that cell retains the state of previous batch\n",
        "#return_sequences to be set as True\n",
        "model.add(CuDNNGRU(units=256, return_sequences=True, stateful=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#Add an GPU based LSTM layer with 750 units\n",
        "model.add(CuDNNGRU(units=256, return_sequences=True, stateful=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#Add an GPU based LSTM layer with 750 units\n",
        "model.add(CuDNNGRU(units=256, stateful=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=vocab_size, activation='softmax', kernel_initializer='he_normal'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 21:55:57.559721 139893707749248 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0717 21:55:57.577175 139893707749248 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0717 21:55:57.579714 139893707749248 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0717 21:55:58.472845 139893707749248 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0717 21:55:58.483350 139893707749248 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0717 21:55:58.987183 139893707749248 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0717 21:55:59.003381 139893707749248 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (32, 128, 8)              392       \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (32, 128, 256)            204288    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (32, 128, 256)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_2 (CuDNNGRU)       (32, 128, 256)            394752    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (32, 128, 256)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_3 (CuDNNGRU)       (32, 256)                 394752    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (32, 256)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (32, 49)                  12593     \n",
            "=================================================================\n",
            "Total params: 1,006,777\n",
            "Trainable params: 1,006,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQtIk1ZHraHN",
        "colab_type": "text"
      },
      "source": [
        "## Training the model and checking the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lHu7KTqr9JC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "35324901-6bd8-4358-d5ff-0d05ec5f0f5b"
      },
      "source": [
        "#Fit the input training dataset to output\n",
        "history = model.fit(x_inp, y_op, batch_size=batch_size, epochs=10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 21:55:59.117868 139893707749248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "386528/386528 [==============================] - 349s 902us/step - loss: 2.6204\n",
            "Epoch 2/10\n",
            "386528/386528 [==============================] - 346s 894us/step - loss: 2.3647\n",
            "Epoch 3/10\n",
            "386528/386528 [==============================] - 346s 895us/step - loss: 2.2799\n",
            "Epoch 4/10\n",
            "386528/386528 [==============================] - 346s 894us/step - loss: 2.2355\n",
            "Epoch 5/10\n",
            "386528/386528 [==============================] - 345s 893us/step - loss: 2.2042\n",
            "Epoch 6/10\n",
            "386528/386528 [==============================] - 345s 892us/step - loss: 2.1826\n",
            "Epoch 7/10\n",
            "386528/386528 [==============================] - 345s 892us/step - loss: 2.1693\n",
            "Epoch 8/10\n",
            "386528/386528 [==============================] - 345s 893us/step - loss: 2.1563\n",
            "Epoch 9/10\n",
            "386528/386528 [==============================] - 345s 893us/step - loss: 2.1478\n",
            "Epoch 10/10\n",
            "386528/386528 [==============================] - 345s 892us/step - loss: 2.1414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5eVxUDnNSPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDSCbq66paAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Helper function to generate text from the model based on the input text\n",
        "def generate_text(pred_text_len=500, inp_arr=[]):\n",
        "  pred_text = np.array([])\n",
        "  inp = np.array(inp_arr[:samp_per_batch]).reshape(1, samp_per_batch)\n",
        "  stuff = np.zeros(shape=(batch_size-1, samp_per_batch))  \n",
        "  inp = np.append(inp, stuff, axis=0)\n",
        "  \n",
        "  for i in range(pred_text_len):\n",
        "    pred = model.predict(inp)\n",
        "    #print (np.argmax(pred[0]))\n",
        "    inp[0] = np.append(inp[0], np.argmax(pred[0]))[1:].reshape(1, samp_per_batch) \n",
        "    pred_text = np.append(pred_text, inp[0][-1]) \n",
        "    \n",
        "    \n",
        "  tx1 = t.sequences_to_texts(np.array(inp_arr[:samp_per_batch]).reshape(-1, 1))\n",
        "  tx1 = ''.join(tx1)\n",
        "\n",
        "  tx = t.sequences_to_texts(pred_text.reshape(-1,1))\n",
        "  tx = ''.join(tx)\n",
        "\n",
        "  tx2 = t.sequences_to_texts(np.array(inp_arr[samp_per_batch:samp_per_batch+pred_text_len]).reshape(-1,1))\n",
        "  tx2 = ''.join(tx2)\n",
        "\n",
        "  print ('----- Input text below -----  ', '\\n', tx1, '\\n ----- Input text end -----\\n\\n')\n",
        "  print ('----- Expected text below -----  ', '\\n', tx2, '\\n ----- Expected text end -----\\n\\n')\n",
        "  print ('----- Predicted text below ----- ', '\\n', tx, '\\n ----- Predicted text end -----\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV8DMbZDzSpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "b532c4d4-32db-4096-88db-85beffd2c7a8"
      },
      "source": [
        "generate_text(500, np.array(tokens[1]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Input text below -----   \n",
            " us charm. he did not venture again until he\n",
            "had found it, and by that time the other boys were tired and ready to\n",
            "rest. they gra \n",
            " ----- Input text end -----\n",
            "\n",
            "\n",
            "----- Expected text below -----   \n",
            " dually wandered apart, dropped into the “dumps,” and\n",
            "fell to gazing longingly across the wide river to where the village lay\n",
            "drowsing in the sun. tom found himself writing “becky” in the sand with\n",
            "his big toe; he scratched it out, and was angry with himself for his\n",
            "weakness. but he wrote it again, nevertheless; he could not help it. he\n",
            "erased it once more and then took himself out of temptation by driving\n",
            "the other boys together and joining them.\n",
            "\n",
            "but joe's spirits had gone down almost beyond re \n",
            " ----- Expected text end -----\n",
            "\n",
            "\n",
            "----- Predicted text below -----  \n",
            "  adan o eee o eee o eee o eee o eee o eee o eee o eee o eeon o o o o o o o o o o o o o o o o o o o o o o o o o o o\n",
            "o oe oe hmr o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eeon o o o o o o o o o o o o o o o o o o o o\n",
            "o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oe oe hmr o eee o eee o eee o eee o eeon o o o o o o\n",
            "o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oe oe hmr o eee o eee o eee o eee o eeon o o o o o  \n",
            " ----- Predicted text end -----\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UkPLTXEzSfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "0a343b06-999a-4b1b-8137-a7b5f5933f9c"
      },
      "source": [
        "generate_text(500, np.array(tokens[2]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Input text below -----   \n",
            " s between ache the harder.\n",
            "\n",
            "becky thatcher was gone to her constantinople home to stay with her\n",
            "parents during vacation--so ther \n",
            " ----- Input text end -----\n",
            "\n",
            "\n",
            "----- Expected text below -----   \n",
            " e was no bright side to life anywhere.\n",
            "\n",
            "the dreadful secret of the murder was a chronic misery. it was a very\n",
            "cancer for permanency and pain.\n",
            "\n",
            "then came the measles.\n",
            "\n",
            "during two long weeks tom lay a prisoner, dead to the world and its\n",
            "happenings. he was very ill, he was interested in nothing. when he got\n",
            "upon his feet at last and moved feebly downtown, a melancholy change had\n",
            "come over everything and every creature. there had been a “revival,” and\n",
            "everybody had “got religion,” not only the adult \n",
            " ----- Expected text end -----\n",
            "\n",
            "\n",
            "----- Predicted text below -----  \n",
            "  ao hsr oe o o o eeodtn o o o o o o o o o o o o oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe\n",
            "hset o o o oe hmr o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eeon o o o o o o o o o o o o o o o o o o o o o o o\n",
            "o oe oe hmr o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eeon o o o o o o o o o o o o o o o o o o o o\n",
            "o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oe oe hmr o eee o eee o eee o eee o eeon o o o o o o\n",
            "o o o o o o \n",
            " ----- Predicted text end -----\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYXRzeT7zSDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "fdb7d5f6-bc66-4c27-af93-2eda7beb70e7"
      },
      "source": [
        "generate_text(500, np.array(tokens[3]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Input text below -----   \n",
            " d wretchedness rise superior to fears in the long run.\n",
            "another tedious wait at the spring and another long sleep brought\n",
            "changes \n",
            " ----- Input text end -----\n",
            "\n",
            "\n",
            "----- Expected text below -----   \n",
            " . the children awoke tortured with a raging hunger. tom believed\n",
            "that it must be wednesday or thursday or even friday or saturday, now,\n",
            "and that the search had been given over. he proposed to explore another\n",
            "passage. he felt willing to risk injun joe and all other terrors. but\n",
            "becky was very weak. she had sunk into a dreary apathy and would not be\n",
            "roused. she said she would wait, now, where she was, and die--it would\n",
            "not be long. she told tom to go with the kite-line and explore if he\n",
            "chose; but \n",
            " ----- Expected text end -----\n",
            "\n",
            "\n",
            "----- Predicted text below -----  \n",
            "  frm hupe ntee o eeon o o o eee o eee o eee o eee o eee o eee o eee o eee o eee o eeon o o o o o o o o o o o o o o o o o o\n",
            "o oe oe hmr o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eeon o o o o o o o o o o o o o o o o o o o o\n",
            "o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oe oe hmr o eee o eee o eee o eee o eeon o o o o o o\n",
            "o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oe oe hmr o eee o eee o eee o eee o eeon o  \n",
            " ----- Predicted text end -----\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs-qopyZzR4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "6c8b75b0-682f-479f-9805-fe92b04c8cde"
      },
      "source": [
        "generate_text(500, np.array(tokens[4]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Input text below -----   \n",
            " s of his great adventure, he\n",
            "noticed that they seemed curiously subdued and far away--somewhat as if\n",
            "they had happened in anothe \n",
            " ----- Input text end -----\n",
            "\n",
            "\n",
            "----- Expected text below -----   \n",
            " r world, or in a time long gone by. then it\n",
            "occurred to him that the great adventure itself must be a dream! there\n",
            "was one very strong argument in favor of this idea--namely, that the\n",
            "quantity of coin he had seen was too vast to be real. he had never seen\n",
            "as much as fifty dollars in one mass before, and he was like all boys of\n",
            "his age and station in life, in that he imagined that all references to\n",
            "“hundreds” and “thousands” were mere fanciful forms of speech, and that\n",
            "no such sums really existed \n",
            " ----- Expected text end -----\n",
            "\n",
            "\n",
            "----- Predicted text below -----  \n",
            "  ao hs o oe o eee o eeon o o o o o o o eee o eee o eee o eee o eee o\n",
            "eeon o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oe oe hmr o eee o eee o eee o eeon o o o o o o\n",
            "o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe hmr\n",
            "o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eee o eeon o o o o o o o o o o o o o o o o o o o o o o o\n",
            "o o o o o o o o o o o o o o o o o o o  \n",
            " ----- Predicted text end -----\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE9KVSY-7OUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}